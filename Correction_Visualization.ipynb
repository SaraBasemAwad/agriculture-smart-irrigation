{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/Users/saraawad/Desktop/Datasets/Google/\"\n",
    "correction_path = \"/Users/saraawad/Desktop/flux-data-qaqc/sites/config/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Helpers:\n",
    "    def __init__(self):\n",
    "        print(\"Helper\")\n",
    "        \n",
    "    def convert_missing_values_nan(df):\n",
    "        '''This function will convert -9999 to NaN'''\n",
    "        df = df.replace(-9999.000000, np.NaN)\n",
    "        return df\n",
    "\n",
    "    def drop_nan_columns(df):\n",
    "        '''Drops the columns having all theirs rows as Nans'''\n",
    "        columns_to_exclude = [\"Date\", \"Day\", \"Year\", \"Month\", \"Timestamp start\"\n",
    "                              , \"Time\", \"TIMESTAMP\", \"Tier\", \"TIMESTAMP_START\", \"TIMESTAMP_END\", \"Day Status\"]\n",
    "        columns = df.columns\n",
    "        for i in range(len(columns)):\n",
    "            col = columns[i]\n",
    "            if col in columns_to_exclude:\n",
    "                continue\n",
    "            nan_sum_col = df[col].isnull().sum()\n",
    "            if nan_sum_col == len(df):\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def drop_nans_rows(df):\n",
    "        '''This function will drop the rows having NaNs'''\n",
    "        print(\"Before removing missing values:\")\n",
    "        print(\"number of rows:\", df.shape[0], \"\\nnumber of columns:\", df.shape[1])\n",
    "        df = df.dropna(how='any')\n",
    "        print(\"After removing missing values:\")\n",
    "        print(\"number of rows:\", df.shape[0], \"\\nnumber of columns:\", df.shape[1])\n",
    "        return df\n",
    "        \n",
    "    def get_all_matching_columns(df, keyword):\n",
    "        return df.filter(like=keyword).columns\n",
    "\n",
    "    def generate_lags(df, column, lags_count): \n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "#             for j in range(i):\n",
    "#                 df.loc[str(j+1), lag_name] = np.nan\n",
    "#         df = df.dropna(how='any')\n",
    "        return df\n",
    "\n",
    "    def add_LE_conversion_rate(df, col):\n",
    "        conversion_rate = 28.94\n",
    "        new_col = col + \"(mm)\"\n",
    "        df[new_col] = df[col] / conversion_rate\n",
    "        return df\n",
    "\n",
    "    def read_sites_data():\n",
    "        file_path = os.path.join(base_path, \"filtered_sites_all.xlsx\")\n",
    "        df = pd.read_excel(file_path)\n",
    "        df.head()\n",
    "        return df\n",
    "\n",
    "    def export_data(df, file_path):\n",
    "        export_path = os.path.join(base_path, file_path + \".csv\")\n",
    "        export_csv = df.to_csv(export_path, index=None, header=True)\n",
    "\n",
    "    def load_data(file_path):\n",
    "        df = pd.read_csv(file_path + \".csv\", delimiter=',')\n",
    "        return df\n",
    "    \n",
    "    def list_to_df(list_to_convert):\n",
    "        '''This function will convert the provided list into a dataframe'''\n",
    "        df = pd.concat(list_to_convert, sort=True)\n",
    "        return df\n",
    "    \n",
    "    def get_files_directory(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "        listOfFile = os.listdir(dirName)\n",
    "        allFiles = list()\n",
    "        # Iterate over all the entries\n",
    "        for entry in listOfFile:\n",
    "            # Create full path\n",
    "            if entry.endswith(\".xlsx\") or entry.endswith(\".icloud\") or entry.endswith(\".DS_Store\"):\n",
    "                continue\n",
    "            fullPath = os.path.join(dirName, entry)\n",
    "            # If entry is a directory then get the list of files in this directory \n",
    "            if os.path.isdir(fullPath):\n",
    "                allFiles = allFiles + Helpers.get_files_directory(fullPath)\n",
    "            else:\n",
    "                allFiles.append(fullPath)\n",
    "\n",
    "        return allFiles\n",
    "\n",
    "    def concat_dataframe_from_files(files, skipRowsNum, split_num):\n",
    "        values = []\n",
    "        for i in range(len(files)):\n",
    "            file_path = files[i]\n",
    "            head, file_name = os.path.split(file_path)\n",
    "            #Get only the sheets having the variables\n",
    "            if file_name.endswith(\".csv\"):\n",
    "#                 print(\"file name\", file_name)\n",
    "                df = pd.read_csv(file_path, delimiter=',', skiprows=skipRowsNum)\n",
    "                site_id = file_name.split(\"_\")[split_num]\n",
    "#                 print(\"site id in file:\", site_id)\n",
    "                df[\"Site Id\"] = site_id\n",
    "                values.append(df)\n",
    "        return Helpers.list_to_df(values)   \n",
    "    \n",
    "    def generate_dataframe_from_files(dirName, skipRowsNum = 0, split_num = 0):\n",
    "        files = Helpers.get_files_directory(dirName)\n",
    "        df = Helpers.concat_dataframe_from_files(files, skipRowsNum, split_num)\n",
    "        return df\n",
    "    \n",
    "    def drop_duplicate_merged_columns(df):\n",
    "        # list comprehension of the cols that end with '_y'\n",
    "        columns_to_drop = [x for x in df if x.endswith('_y')]\n",
    "        df.drop(columns_to_drop, axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectedData:\n",
    "\n",
    "    def __init__(self, input_path, output_path):\n",
    "        print(\"Initializer\")\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        \n",
    "    def concat_sites(self, path):\n",
    "        all_ebr_list = []\n",
    "        files = Helpers.get_files_directory(path)\n",
    "        for i in range(len(files)):\n",
    "            file_path = files[i]\n",
    "            head, file_name = os.path.split(file_path)\n",
    "            df_site = pd.read_csv(file_path, delimiter=',', skiprows=0)\n",
    "            site_id = file_name.split(\"_\")[0]\n",
    "            if file_name.endswith(\"daily_data.csv\"):\n",
    "                df_site[\"Site Id\"] = site_id\n",
    "                all_ebr_list.append(df_site)\n",
    "            \n",
    "        return Helpers.list_to_df(all_ebr_list)   \n",
    "        \n",
    "    def merge_correction_methods(self):\n",
    "        ebr_path = os.path.join(self.input_path, \"EBR/\")\n",
    "        bowen_path = os.path.join(self.input_path, \"Bowen/\")\n",
    "        df_ebr = self.concat_sites(ebr_path)\n",
    "        df_ebr = df_ebr.rename({'LE_corr': 'LE_ebr_corr', 'H_corr': 'H_ebr_corr', \n",
    "                           'ET': 'ET_ebr', 'ET_corr': 'ET_ebr_corr', \n",
    "                           'ETrF': 'ETrF_ebr', 'ETrF_filtered': 'ETrF_filtered_ebr'}, axis=1) \n",
    "        df_bowen = self.concat_sites(bowen_path)\n",
    "        df_bowen = df_bowen.rename({'LE_corr': 'LE_bowen_corr', 'H_corr': 'H_bowen_corr', \n",
    "                           'ET':'ET_bowen', 'ET_corr': 'ET_bowen_corr', \n",
    "                           'ETrF': 'ETrF_bowen', 'ETrF_filtered': 'ETrF_filtered_bowen'}, axis=1) \n",
    "        df_all = pd.merge(left=df_ebr,right=df_bowen, left_on=['Site Id', 'date'], \n",
    "                          right_on=['Site Id', 'date'], suffixes=('', '_y'))\n",
    "        Helpers.drop_duplicate_merged_columns(df_all)\n",
    "        \n",
    "        file_name = self.output_path + \"all_sites_corrected\"\n",
    "        Helpers.export_data(df_all, file_name) \n",
    "        \n",
    "        return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output_path = os.path.join(correction_path, \"Merged/\")\n",
    "    am = CorrectedData(correction_path, output_path)\n",
    "    df_all = am.merge_correction_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US-Ced', 'US-A74', 'US-Twt', 'US-Var', 'US-SO2', 'US-Kon',\n",
       "       'US-Tw2', 'US-Pon', 'US-Bi2', 'US-A32', 'US-AR1', 'US-Goo',\n",
       "       'US-SP2', 'US-Snd', 'US-Wlr', 'US-Skr', 'US-AR2', 'US-Shd'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"Site Id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_LE</th>\n",
       "      <th>LE_bowen_corr</th>\n",
       "      <th>LE_ebr_corr</th>\n",
       "      <th>input_H</th>\n",
       "      <th>H_bowen_corr</th>\n",
       "      <th>H_ebr_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>76.165051</td>\n",
       "      <td>70.411027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.693556</td>\n",
       "      <td>70.612895</td>\n",
       "      <td>61.744799</td>\n",
       "      <td>77.398378</td>\n",
       "      <td>90.047839</td>\n",
       "      <td>78.738957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107.135289</td>\n",
       "      <td>127.536709</td>\n",
       "      <td>108.658759</td>\n",
       "      <td>85.455467</td>\n",
       "      <td>101.728469</td>\n",
       "      <td>86.670648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.954978</td>\n",
       "      <td>128.281493</td>\n",
       "      <td>135.484160</td>\n",
       "      <td>102.333911</td>\n",
       "      <td>97.273529</td>\n",
       "      <td>102.735180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138.517261</td>\n",
       "      <td>136.322478</td>\n",
       "      <td>138.128031</td>\n",
       "      <td>86.831239</td>\n",
       "      <td>85.455413</td>\n",
       "      <td>86.587246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22369</th>\n",
       "      <td>NaN</td>\n",
       "      <td>75.903437</td>\n",
       "      <td>39.753796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22370</th>\n",
       "      <td>93.100000</td>\n",
       "      <td>108.739620</td>\n",
       "      <td>116.387749</td>\n",
       "      <td>33.100000</td>\n",
       "      <td>38.660380</td>\n",
       "      <td>41.379533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22371</th>\n",
       "      <td>-21.700000</td>\n",
       "      <td>-487.165000</td>\n",
       "      <td>-26.323984</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>576.965000</td>\n",
       "      <td>31.176332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22372</th>\n",
       "      <td>NaN</td>\n",
       "      <td>203.147222</td>\n",
       "      <td>127.504853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22373</th>\n",
       "      <td>309.100000</td>\n",
       "      <td>310.008405</td>\n",
       "      <td>333.577335</td>\n",
       "      <td>201.300000</td>\n",
       "      <td>201.891595</td>\n",
       "      <td>217.240755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22374 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         input_LE  LE_bowen_corr  LE_ebr_corr     input_H  H_bowen_corr  \\\n",
       "0             NaN      76.165051    70.411027         NaN           NaN   \n",
       "1       60.693556      70.612895    61.744799   77.398378     90.047839   \n",
       "2      107.135289     127.536709   108.658759   85.455467    101.728469   \n",
       "3      134.954978     128.281493   135.484160  102.333911     97.273529   \n",
       "4      138.517261     136.322478   138.128031   86.831239     85.455413   \n",
       "...           ...            ...          ...         ...           ...   \n",
       "22369         NaN      75.903437    39.753796         NaN           NaN   \n",
       "22370   93.100000     108.739620   116.387749   33.100000     38.660380   \n",
       "22371  -21.700000    -487.165000   -26.323984   25.700000    576.965000   \n",
       "22372         NaN     203.147222   127.504853         NaN           NaN   \n",
       "22373  309.100000     310.008405   333.577335  201.300000    201.891595   \n",
       "\n",
       "       H_ebr_corr  \n",
       "0             NaN  \n",
       "1       78.738957  \n",
       "2       86.670648  \n",
       "3      102.735180  \n",
       "4       86.587246  \n",
       "...           ...  \n",
       "22369         NaN  \n",
       "22370   41.379533  \n",
       "22371   31.176332  \n",
       "22372         NaN  \n",
       "22373  217.240755  \n",
       "\n",
       "[22374 rows x 6 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(10)\n",
    "df_all[[\"input_LE\", \"LE_bowen_corr\", \"LE_ebr_corr\", \"input_H\", \"H_bowen_corr\", \"H_ebr_corr\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmerifluxErrorGraph:\n",
    "    def __init__(self, path):\n",
    "        print(\"AmerifluxErrorGraph Initializer\")\n",
    "        self.path = path\n",
    "        \n",
    "          \n",
    "    def get_error_metrics(self, y_true, y_predicted):\n",
    "        r2_Score = r2_score(y_true, y_predicted)\n",
    "        rmse_score = np.sqrt(mean_squared_error(y_true, y_predicted))\n",
    "        mse_score = mean_squared_error(y_true, y_predicted)\n",
    "        mae_score = mean_absolute_error(y_true, y_predicted)\n",
    "\n",
    "        def mean_absolute_percentage_error(y_true, y_pred):\n",
    "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mape_score = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "        num = 2\n",
    "        return (round(r2_Score, num), round(rmse_score, num), round(mse_score, num), \n",
    "                round(mae_score, num), round(mape_score, num))\n",
    "\n",
    "\n",
    "    def generate_errors(self, df, first_col, second_col):\n",
    "        df = df.replace(to_replace = np.nan, value =0) \n",
    "        errors = self.get_error_metrics(df[first_col], df[second_col])\n",
    "        return errors\n",
    "    \n",
    "    def plot_et(self, df, site_id, first_column, first_b_column, second_column, path_to_save):\n",
    "        fig, ax = plt.subplots(figsize=(45, 22))\n",
    "        fig.subplots_adjust(bottom=0.15, left=0.2)\n",
    "        plt.subplot(121)\n",
    "        title1 = site_id + \": Comparison between \" + first_column + \" and \" + second_column\n",
    "        plt.rcParams.update({'font.size': 20})\n",
    "        self.plot_sub(df, first_column, second_column, title1)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.rcParams.update({'font.size': 20})\n",
    "        title2 = site_id + \": Comparison between \" + first_b_column + \" and \" + second_column\n",
    "        self.plot_sub(df, first_b_column, second_column, title2)\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "#         ax.legend()\n",
    "#         plt.show()\n",
    "        plt.savefig(path_to_save + site_id + \"_corr.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    def plot_sub(self, df, first_column, second_column, title):\n",
    "        ax = sns.regplot(x=first_column, y=second_column, data=df, fit_reg=False)\n",
    "        lims = [\n",
    "            np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "            np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "            ]\n",
    "\n",
    "        # now plot both limits against eachother\n",
    "        ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(lims)\n",
    "        ax.set_ylim(lims)\n",
    "       \n",
    "        plt.title(title)\n",
    "        \n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        unique_sites = df[\"Site Id\"].unique()\n",
    "        unique_sites\n",
    "\n",
    "        r2_list, rmse_list, mse_list, mae_list, mape_list = [], [], [], [], []\n",
    "        r2_b_list, rmse_b_list, mse_b_list, mae_b_list, mape_b_list = [], [], [], [], []\n",
    "        first_column = \"LE_bowen_corr\"\n",
    "        first_b_column = \"LE_ebr_corr\"\n",
    "\n",
    "        second_column = \"input_LE\"\n",
    "        for i in range(len(unique_sites)):\n",
    "            site_id = unique_sites[i]\n",
    "            df_site = df[df[\"Site Id\"] == site_id]\n",
    "            (r2, rmse, mse, mae, mape) = self.generate_errors(df_site, first_column, second_column)\n",
    "            (r2_b, rmse_b, mse_b, mae_b, mape_b) = self.generate_errors(df_site, first_b_column, second_column)\n",
    "            r2_list.append(r2)\n",
    "            rmse_list.append(rmse)\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "            \n",
    "            r2_b_list.append(r2_b)\n",
    "            rmse_b_list.append(rmse_b)\n",
    "            mse_b_list.append(mse_b)\n",
    "            mae_b_list.append(mae_b)\n",
    "            mape_b_list.append(mape_b)\n",
    "            df_site['date'] = pd.to_datetime(df_site[\"date\"])\n",
    "            df_site.sort_values(by=\"date\", inplace=True, ascending=True)\n",
    "            #Plot the output feature\n",
    "            if len(df_site) > 0:\n",
    "                self.plot_et(df_site, site_id, first_column, first_b_column, second_column, self.path + \"Graphs/\")\n",
    "\n",
    "        df_errors = pd.DataFrame({\"Site Id\": unique_sites,\n",
    "                                  \"LE\": second_column,\n",
    "                                  \"LE Bowen\": first_column,\n",
    "                                  \"R2\": r2_list,\n",
    "                                  \"RMSE\": rmse_list, \n",
    "                                  \"MSE\": rmse_list,\n",
    "                                  \"MAE\": mae_list, \n",
    "                                  \"MAPE\": mape_list,\n",
    "                                  \"LE Ebr\": first_b_column,\n",
    "                                  \"R2 ebr\": r2_b_list,\n",
    "                                  \"RMSE ebr\": rmse_b_list, \n",
    "                                  \"MSE ebr\": mse_b_list,\n",
    "                                  \"MAE ebr\": mae_b_list, \n",
    "                                  \"MAPE ebr\": mape_b_list})\n",
    "        \n",
    "        print(df_errors)\n",
    " \n",
    "        Helpers.export_data(df_errors, self.path + \"corrections_errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmerifluxErrorGraph Initializer\n",
      "   Site Id        LE       LE Bowen    R2    RMSE     MSE     MAE    MAPE  \\\n",
      "0   US-Ced  input_LE  LE_bowen_corr -1.24   52.41   52.41   39.94  108.85   \n",
      "1   US-A74  input_LE  LE_bowen_corr -0.65  171.28  171.28  109.32   70.78   \n",
      "2   US-Twt  input_LE  LE_bowen_corr -0.85   97.95   97.95   70.02     NaN   \n",
      "3   US-Var  input_LE  LE_bowen_corr  0.74   33.53   33.53   19.55   56.28   \n",
      "4   US-SO2  input_LE  LE_bowen_corr  0.85   14.04   14.04    9.68   34.87   \n",
      "5   US-Kon  input_LE  LE_bowen_corr  0.40   46.69   46.69   28.36   89.45   \n",
      "6   US-Tw2  input_LE  LE_bowen_corr  0.53   42.79   42.79   30.40   35.26   \n",
      "7   US-Pon  input_LE  LE_bowen_corr -2.10  141.22  141.22  122.11   84.58   \n",
      "8   US-Bi2  input_LE  LE_bowen_corr  0.07   64.92   64.92   41.58   64.80   \n",
      "9   US-A32  input_LE  LE_bowen_corr  0.26   74.34   74.34   43.68   41.85   \n",
      "10  US-AR1  input_LE  LE_bowen_corr  0.03   31.86   31.86   19.25   85.58   \n",
      "11  US-Goo  input_LE  LE_bowen_corr  0.46   58.84   58.84   37.32     NaN   \n",
      "12  US-SP2  input_LE  LE_bowen_corr -0.28  246.25  246.25  114.81   91.43   \n",
      "13  US-Snd  input_LE  LE_bowen_corr  0.26   37.21   37.21   25.21   55.00   \n",
      "14  US-Wlr  input_LE  LE_bowen_corr -0.87   73.62   73.62   53.37     NaN   \n",
      "15  US-Skr  input_LE  LE_bowen_corr -0.63   79.00   79.00   59.70   52.91   \n",
      "16  US-AR2  input_LE  LE_bowen_corr -0.42   26.79   26.79   18.83     NaN   \n",
      "17  US-Shd  input_LE  LE_bowen_corr  0.26  132.59  132.59   62.07   63.02   \n",
      "\n",
      "         LE Ebr  R2 ebr  RMSE ebr   MSE ebr  MAE ebr  MAPE ebr  \n",
      "0   LE_ebr_corr   -1.52     53.43   2855.11    41.83     91.80  \n",
      "1   LE_ebr_corr   -0.70    165.55  27406.69   105.33     65.49  \n",
      "2   LE_ebr_corr   -0.85     96.77   9364.94    69.22       NaN  \n",
      "3   LE_ebr_corr    0.82     27.23    741.33    18.36     29.90  \n",
      "4   LE_ebr_corr    0.91     11.18    125.02     7.08     16.21  \n",
      "5   LE_ebr_corr    0.58     39.78   1582.42    22.91     44.84  \n",
      "6   LE_ebr_corr    0.58     37.22   1385.70    29.03     34.54  \n",
      "7   LE_ebr_corr   -2.23    144.08  20758.78   123.96     83.93  \n",
      "8   LE_ebr_corr    0.08     63.82   4072.89    40.88     62.84  \n",
      "9   LE_ebr_corr    0.26     74.32   5522.88    41.04     38.65  \n",
      "10  LE_ebr_corr    0.02     29.97    898.13    18.38     57.99  \n",
      "11  LE_ebr_corr    0.46     56.93   3240.56    36.23       NaN  \n",
      "12  LE_ebr_corr   -1.35    136.98  18762.75   104.55     67.18  \n",
      "13  LE_ebr_corr    0.26     33.87   1147.30    23.30     46.91  \n",
      "14  LE_ebr_corr   -0.83     73.07   5339.08    52.36       NaN  \n",
      "15  LE_ebr_corr   -0.36     77.19   5958.33    54.62     45.64  \n",
      "16  LE_ebr_corr   -0.40     25.83    667.43    17.86       NaN  \n",
      "17  LE_ebr_corr    0.40     90.54   8197.59    54.98     60.31  \n"
     ]
    }
   ],
   "source": [
    "full_path = os.path.join(base_path, \"Ameriflux/Generated/\")\n",
    "am_eg = AmerifluxErrorGraph(full_path)\n",
    "am_eg.prepare_data(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
