{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import refet\n",
    "import pvlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Global Variables </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/Users/saraawad/Desktop/Datasets/Google/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Helpers:\n",
    "    def __init__(self):\n",
    "        print(\"Helper\")\n",
    "\n",
    "    def drop_nan_columns(df):\n",
    "        \"\"\"Drops the columns having all theirs rows as Nans\"\"\"\n",
    "        columns_to_exclude = [\"Date\", \"Day\", \"Year\", \"Month\", \"Timestamp start\"\n",
    "                              , \"Time\", \"TIMESTAMP\", \"Tier\", \"TIMESTAMP_START\", \"TIMESTAMP_END\", \"Day Status\"]\n",
    "        columns = df.columns\n",
    "        for i in range(len(columns)):\n",
    "            col = columns[i]\n",
    "            if col in columns_to_exclude:\n",
    "                continue\n",
    "            nan_sum_col = df[col].isnull().sum()\n",
    "            if nan_sum_col == len(df):\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def convert_missing_values_nan(df):\n",
    "        '''This function will convert -9999 to NaN'''\n",
    "        df = df.replace(-9999.000000, np.NaN)\n",
    "        return df\n",
    "\n",
    "    def get_all_matching_columns(df, keyword):\n",
    "        return df.filter(like=keyword).columns\n",
    "\n",
    "    def generate_lags(df, column, lags_count): \n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "            for j in range(i):\n",
    "                df.loc[str(j+1), lag_name] = np.nan\n",
    "        return df\n",
    "\n",
    "    def add_LE_conversion_rate(df, col):\n",
    "        conversion_rate = 28.94\n",
    "        new_col = col + \"(mm)\"\n",
    "        df[new_col] = df[col] / conversion_rate\n",
    "        return df\n",
    "\n",
    "    def read_sites_data():\n",
    "        file_path = os.path.join(base_path, \"filtered_sites_all.xlsx\")\n",
    "        df = pd.read_excel(file_path)\n",
    "        df.head()\n",
    "        return df\n",
    "\n",
    "    def export_data(df, file_path):\n",
    "        export_path = os.path.join(base_path, file_path + \".csv\")\n",
    "        export_csv = df.to_csv(export_path, index=None, header=True)\n",
    "\n",
    "    def load_data(file_path):\n",
    "        df = pd.read_csv(file_path + \".csv\", delimiter=',')\n",
    "        return df\n",
    "    \n",
    "    def list_to_df(list_to_convert):\n",
    "        '''This function will convert the provided list into a dataframe'''\n",
    "        df = pd.concat(list_to_convert, sort=True)\n",
    "        return df\n",
    "    \n",
    "    def get_files_directory(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "        listOfFile = os.listdir(dirName)\n",
    "        allFiles = list()\n",
    "        # Iterate over all the entries\n",
    "        for entry in listOfFile:\n",
    "            # Create full path\n",
    "            if entry.endswith(\".xlsx\") or entry.endswith(\".icloud\") or entry.endswith(\".DS_Store\"):\n",
    "                continue\n",
    "            fullPath = os.path.join(dirName, entry)\n",
    "            # If entry is a directory then get the list of files in this directory \n",
    "            if os.path.isdir(fullPath):\n",
    "                allFiles = allFiles + Helpers.get_files_directory(fullPath)\n",
    "            else:\n",
    "                allFiles.append(fullPath)\n",
    "\n",
    "        return allFiles\n",
    "\n",
    "    def concat_dataframe_from_files(files):\n",
    "        values = []\n",
    "        for i in range(len(files)):\n",
    "            file_path = files[i]\n",
    "            head, file_name = os.path.split(file_path)\n",
    "            #Get only the sheets having the variables\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                print(\"file name\", file_name)\n",
    "                df = pd.read_csv(file_path, delimiter=',')\n",
    "                site_id = file_name.split(\"_\")[0]\n",
    "                df[\"Site Id\"] = site_id\n",
    "                values.append(df)\n",
    "        return Helpers.list_to_df(values)   \n",
    "    \n",
    "    def generate_joint_dataframe(dirName):\n",
    "        files = Helpers.get_files_directory(dirName)\n",
    "        df = Helpers.concat_dataframe_from_files(files)\n",
    "        return df\n",
    "    \n",
    "    def string_to_date(df, column, dateFormat=\"%Y%m%d%H%M\"):\n",
    "        df[column] = pd.to_datetime(df[column], format=dateFormat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Data Initializer\n",
      "file name US-Shd_EEflux_ETo.csv\n",
      "file name US-KFS_EEflux_ET0 2.csv\n",
      "file name US-FR2_EEflux_ET0.csv\n",
      "file name US-Wgr_EEflux_ETo 2.csv\n",
      "file name US-Wlr_EEflux_ETo.csv\n",
      "file name US-Skr_EEflux_ETo.csv\n",
      "file name US-Myb_EEflux_ET0.csv\n",
      "file name US-WBW_EEflux_ETo.csv\n",
      "file name US-KFS_EEflux_ETo.csv\n",
      "file name US-AR2_EEflux_ETo.csv\n",
      "file name US-SP2_EEflux_ETo.csv\n",
      "file name US-Me6_EEflux_ETo.csv\n",
      "file name US-A32_EEflux_ETo.csv\n",
      "file name US-Var_EEflux_ET0.csv\n",
      "file name US-Kon_EEflux_ET0.csv\n",
      "file name US-Goo_EEflux_ETo.csv\n",
      "file name US-ARM_EEflux_ETo.csv\n",
      "file name US-Pon_EEflux_ETo.csv\n",
      "file name US-Snd_EEflux_ET0.csv\n",
      "file name US-Bi2_EEflux_ETo.csv\n",
      "file name US-Wgr_EEflux_ETo.csv\n",
      "file name US-Twt_EEflux_ETo.csv\n",
      "file name US-AR1_EEflux_ET0.csv\n",
      "file name US-SO2_EEflux_ETo.csv\n",
      "file name US-Tw3_EEflux_ETo.csv\n",
      "file name US-Me2_EEflux_ET0.csv\n",
      "file name US-Tw1_EEflux_ETo.csv\n",
      "file name US-A74_EEflux_ETo.csv\n",
      "file name US-MMS_EEflux_ET0.csv\n",
      "file name US-AR2_EEflux_ETo 2.csv\n",
      "file name US-Ced_EEflux_ETo.csv\n",
      "file name US-Myb_EEflux_ETo.csv\n",
      "file name US-KFS_EEflux_ET0.csv\n"
     ]
    }
   ],
   "source": [
    "class AggregateData:\n",
    "    def __init__(self, path):\n",
    "        print(\"Aggregate Data Initializer\")\n",
    "        self.path = path\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        folder_path = os.path.join(base_path, self.path)\n",
    "        df = Helpers.generate_joint_dataframe(folder_path)\n",
    "        return df\n",
    "\n",
    "#Change path in here to read reference evapotranspiration ET from\n",
    "path = \"EEflux/ETr/\"\n",
    "am = AggregateData(path)\n",
    "df = am.prepare_data()\n",
    "df.head()\n",
    "\n",
    "\n",
    "df = df[[\"Date\", \"Site Id\", \"Cloud\", \"Image Id\", \"ETo\", \"Mean ETo\"]]\n",
    "#Save data in path\n",
    "Helpers.export_data(df, \"EEflux_refET_sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmerifluxHourlyRefET Initializer\n",
      "files count: 32\n",
      "Site Id: US-Tw1\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (3111, 13)\n",
      "merged df (643, 18)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Skr\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (2922, 9)\n",
      "merged df (714, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Snd\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (2922, 9)\n",
      "merged df (342, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Twt\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH_PI_F\n",
      "daily ameriflux df (3016, 13)\n",
      "merged df (660, 18)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Bi2\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (730, 13)\n",
      "merged df (227, 18)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Myb\n",
      "No rs\n",
      "ws: WS rs: [] tmean: TA rh: RH\n",
      "daily ameriflux df (3516, 13)\n",
      "merged df (1070, 18)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Ton\n",
      "ws: WS_1_1_1 rs: SW_IN_1_1_1 tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (6770, 15)\n",
      "merged df (0, 20)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-HRC\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (238, 11)\n",
      "merged df (0, 16)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-FR2\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1461, 9)\n",
      "merged df (149, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-MMS\n",
      "ws: WS_1_1_1 rs: SW_IN_1_1_1 tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (7397, 9)\n",
      "merged df (1337, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Pon\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1461, 9)\n",
      "merged df (265, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Me2\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_1 rh: RH\n",
      "daily ameriflux df (6058, 9)\n",
      "merged df (655, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Ced\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (3652, 9)\n",
      "merged df (820, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Kon\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (2557, 9)\n",
      "merged df (460, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-KFS\n",
      "ws: WS_1_1_1 rs: SW_IN_1_1_1 tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (4018, 15)\n",
      "merged df (1275, 20)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-ARM\n",
      "ws: WS_1_1_1 rs: SW_IN_1_1_1 tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (5845, 11)\n",
      "merged df (769, 16)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Wlr\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1369, 9)\n",
      "merged df (286, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-AR2\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (1461, 9)\n",
      "merged df (740, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Tw2\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (731, 9)\n",
      "merged df (0, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-A32\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1096, 9)\n",
      "merged df (276, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Wgr\n",
      "no rh\n",
      "Site Id: US-WBW\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (4748, 9)\n",
      "merged df (101, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Tw3\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1981, 13)\n",
      "merged df (487, 18)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Shd\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1461, 9)\n",
      "merged df (281, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-SP2\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (3653, 9)\n",
      "merged df (384, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-HRA\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (238, 11)\n",
      "merged df (0, 16)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Var\n",
      "ws: WS rs: SW_IN_1_1_1 tmean: TA rh: RH\n",
      "daily ameriflux df (7136, 11)\n",
      "merged df (1724, 16)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Me6\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_2 rh: RH\n",
      "daily ameriflux df (3226, 9)\n",
      "merged df (360, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-Goo\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1826, 9)\n",
      "merged df (368, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-SO2\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (3652, 9)\n",
      "merged df (700, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-A74\n",
      "ws: WS rs: SW_IN tmean: TA rh: RH\n",
      "daily ameriflux df (1096, 9)\n",
      "merged df (186, 14)\n",
      "-----------------------------------------------------------------\n",
      "Site Id: US-AR1\n",
      "ws: WS rs: SW_IN tmean: TA_1_1_1 rh: RH_1_1_1\n",
      "daily ameriflux df (1461, 9)\n",
      "merged df (141, 14)\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class AmerifluxHourlyRefET:\n",
    "    def __init__(self, path, eeflux_path):\n",
    "        print(\"AmerifluxHourlyRefET Initializer\")\n",
    "        self.path = path\n",
    "        self.eeflux_path = eeflux_path\n",
    "        \n",
    "        \n",
    "    def read_data_sites_info(self, site_id):\n",
    "        file_path = os.path.join(base_path, \"filtered_sites_all.xlsx\")\n",
    "        sites_df = pd.read_excel(file_path)\n",
    "        site_df = sites_df[sites_df[\"Site Id\"] == site_id]\n",
    "        return site_df\n",
    "        \n",
    "    def set_coordinates(self, df, site_id):\n",
    "        site_df = self.read_data_sites_info(site_id)\n",
    "        elevation = site_df.iloc[0,3]\n",
    "        latitude = site_df.iloc[0,1]\n",
    "        longitude = site_df.iloc[0,2]\n",
    "        df[\"elevation\"] = elevation\n",
    "        df[\"latitude\"] = latitude\n",
    "        df[\"longitude\"] = longitude\n",
    "        return df\n",
    "        \n",
    "    def get_day_of_year(self, df):\n",
    "        date_time_index = df.columns.get_loc(\"TIMESTAMP_END\")\n",
    "        day_of_year_list = []\n",
    "        times = []\n",
    "        hours = []\n",
    "        for i in range(len(df)):\n",
    "            date_str = str(df.iloc[i, date_time_index])\n",
    "            date = datetime.datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "            day_of_year = date.timetuple().tm_yday\n",
    "            day_of_year_list.append(day_of_year)\n",
    "\n",
    "            date_time = date.time()\n",
    "            hour = int(str(date_time).split(\":\")[0])\n",
    "            \n",
    "            times.append(date_time)\n",
    "            hours.append(hour)\n",
    "\n",
    "        df[\"doy\"] = day_of_year_list\n",
    "        df[\"time\"] = times\n",
    "        df[\"hour\"] = hours\n",
    "        \n",
    "    def add_LE_conversion_rate(self, df, col):\n",
    "        conversion_rate = 28.94\n",
    "        new_col = col + \"(mm)\"\n",
    "        df[new_col] = df[col] / conversion_rate\n",
    "        return df\n",
    "    \n",
    "    def add_LE_converstion_to_LE(self, df):\n",
    "        le_columns = list(Helpers.get_all_matching_columns(df, \"LE_\"))\n",
    "        columns_list = []\n",
    "        if \"LE\" in df.columns:\n",
    "            columns_list.append(\"LE\")\n",
    "        columns_list.extend(le_columns)\n",
    "\n",
    "        for k in range(len(columns_list)):\n",
    "            col = columns_list[k]\n",
    "            df = Helpers.add_LE_conversion_rate(df, col)\n",
    "        return df\n",
    "        \n",
    "    def set_ref_ET_values(self, df):\n",
    "        df[\"ea\"] = \"\"\n",
    "        solar_rad_col = list(Helpers.get_all_matching_columns(df, \"SW_IN\"))\n",
    "        if len(solar_rad_col) > 0:\n",
    "            solar_rad_col = solar_rad_col[0]\n",
    "            df[\"rs\"] = df[solar_rad_col] * 0.0036\n",
    "        else:\n",
    "            print(\"No rs\")\n",
    "            df[\"rs\"] = pvlib.irradiance.get_extra_radiation(df[\"doy\"], 0.0820 ,'asce')\n",
    "        ws_col = list(Helpers.get_all_matching_columns(df, \"WS\"))[0]\n",
    "        df[\"uz\"] = np.where(df[ws_col].isnull(), 2, df[ws_col])\n",
    "        if \"TA\" in df.columns:\n",
    "            ta_col = \"TA\"\n",
    "        else:\n",
    "            ta_col = list(Helpers.get_all_matching_columns(df, \"TA_\"))[0]\n",
    "        df[\"tmean\"] = df[ta_col]\n",
    "        rh_col = list(Helpers.get_all_matching_columns(df, \"RH\"))\n",
    "        \n",
    "        if len(rh_col) > 0 :\n",
    "            rh_col = rh_col[0]\n",
    "            df[\"rh\"] = df[rh_col]\n",
    "        else:\n",
    "            print(\"no rh\")\n",
    "            return\n",
    "    \n",
    "        print(\"ws:\", ws_col, \"rs:\", solar_rad_col, \"tmean:\", ta_col, \"rh:\", rh_col)\n",
    "        zw = 2\n",
    "        df[\"zw\"] = zw\n",
    "        df[\"etr\"] = \"\"\n",
    "        \n",
    "        elev_index = df.columns.get_loc(\"elevation\")\n",
    "        lat_index = df.columns.get_loc(\"latitude\")\n",
    "        lon_index = df.columns.get_loc(\"longitude\")\n",
    "        ea_index = df.columns.get_loc(\"ea\")\n",
    "        uz_index = df.columns.get_loc(\"uz\")\n",
    "        rs_index = df.columns.get_loc(\"rs\")\n",
    "        rh_index = df.columns.get_loc(\"rh\")\n",
    "        doy_index = df.columns.get_loc(\"doy\")\n",
    "        tmean_index = df.columns.get_loc(\"tmean\")\n",
    "        time_index = df.columns.get_loc(\"time\")\n",
    "        hour_index = df.columns.get_loc(\"hour\")\n",
    "        etr_index = df.columns.get_loc(\"etr\")\n",
    "        \n",
    "        es = 0.6108 * np.exp(17.27 * df[\"tmean\"] / (df[\"tmean\"] + 237.3))\n",
    "        ea = df[\"rh\"] / 100 * es\n",
    "        df[\"ea\"] = ea\n",
    "\n",
    "        etr = refet.Hourly(tmean= df[\"tmean\"], ea=df[\"ea\"] , rs=df[\"rs\"],\n",
    "                           uz=df[\"uz\"], zw=df[\"zw\"], elev=df[\"elevation\"],\n",
    "                           lat=df[\"latitude\"], lon=df[\"longitude\"], doy=df[\"doy\"],\n",
    "                           time=df[\"hour\"], method='asce').etr()\n",
    "        df[\"etr\"] = etr\n",
    "\n",
    "        \n",
    "    def resample_date_todays(self, df, dictionary):\n",
    "        time_stamp = []\n",
    "        df[\"Date\"] = \"\"\n",
    "        for i in range(df.shape[0]):\n",
    "            index_date_column = df.columns.get_loc(\"TIMESTAMP_START\")\n",
    "            date = datetime.datetime.strptime(str(df.iloc[i, index_date_column]),\n",
    "                                              \"%Y-%m-%d %H:%M:%S\").strftime('%m/%d/%y')\n",
    "            time_stamp.append(date)\n",
    "        df[\"Date\"] = time_stamp\n",
    "        df = df.groupby(['Date'], as_index=False).agg(dictionary)\n",
    "        print(\"daily ameriflux df\", df.shape)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def read_eeflux(self, eeflux_path, site_id):\n",
    "        file_path = os.path.join(base_path, eeflux_path)\n",
    "        df = pd.read_csv(file_path, delimiter=',')\n",
    "        df = df[df[\"Site Id\"] == site_id]\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        return df\n",
    "\n",
    "    def merge_eeflux_ameriflux_refET(self, eeflux_df, am_df):\n",
    "        df = pd.merge(eeflux_df, am_df, left_on=[\"Date\"], \n",
    "                         right_on=[\"Date\"], how=\"inner\")\n",
    "        print(\"merged df\", df.shape)\n",
    "        return df\n",
    "    \n",
    "    def generate_dictionary(self, df):\n",
    "        sum_columns = []\n",
    "        if \"LE\" in df.columns:\n",
    "            sum_columns.append(\"LE\")\n",
    "        sum_columns.extend(list(Helpers.get_all_matching_columns(df, \"LE_\")))\n",
    "        sum_columns.extend(list(Helpers.get_all_matching_columns(df, \"mm\")))\n",
    "        \n",
    "        dictionary_input = {\n",
    "        \"rh\": np.mean,\n",
    "        \"tmean\": np.mean,\n",
    "        \"rs\": np.mean,\n",
    "        \"ea\": np.mean,\n",
    "        \"uz\": np.mean,\n",
    "        \"etr\": sum\n",
    "        }\n",
    "\n",
    "        dictionary_output = {\n",
    "            i : sum for i in sum_columns\n",
    "        }\n",
    "        result_dictionary = {**dictionary_input , **dictionary_output}\n",
    "        return result_dictionary\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        folder_path = os.path.join(base_path, self.path)\n",
    "        files = Helpers.get_files_directory(folder_path)\n",
    "        print(\"files count:\", len(files))\n",
    "        all_merged_dfs = []\n",
    "        for i in range(len(files)):\n",
    "            file_path = files[i]\n",
    "            head, file_name = os.path.split(file_path)\n",
    "            if file_name.endswith(\".csv\"):\n",
    "            #Skip first 2 lines, meaningless data\n",
    "                df = pd.read_csv(file_path, delimiter=',', skiprows=2)\n",
    "                site_id = file_name.split(\"_\")[1]\n",
    "                print(\"Site Id:\", site_id)\n",
    "                df[\"Site Id\"] = site_id\n",
    "                df = df.replace(-9999.000000, np.NaN)\n",
    "                df = Helpers.drop_nan_columns(df)\n",
    "                Helpers.string_to_date(df, \"TIMESTAMP_START\")\n",
    "                Helpers.string_to_date(df, \"TIMESTAMP_END\")\n",
    "                self.get_day_of_year(df)\n",
    "                df = self.add_LE_converstion_to_LE(df)\n",
    "                df = self.set_coordinates(df, site_id)\n",
    "                self.set_ref_ET_values(df)\n",
    "                if \"rh\" not in df.columns:\n",
    "                    continue\n",
    "                dictionary = self.generate_dictionary(df)\n",
    "                df = self.resample_date_todays(df, dictionary)\n",
    "                df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%y\")\n",
    "                eeflux_df = self.read_eeflux(self.eeflux_path, site_id)\n",
    "                merged_df = self.merge_eeflux_ameriflux_refET(eeflux_df, df)\n",
    "                merged_df[\"ETo EEflux\"] = np.where(merged_df[\"ETo\"].isnull(), merged_df[\"Mean ETo\"],\n",
    "                                 merged_df[\"ETo\"])\n",
    "                ameriflux_eeflux_path = \"Ameriflux/Generated/v1/Ameriflux EEflux RefET/\" + site_id + \"_Ameriflux_EEflux_refET\"\n",
    "                Helpers.export_data(merged_df, ameriflux_eeflux_path)\n",
    "                all_merged_dfs.append(merged_df)\n",
    "                print(\"-----------------------------------------------------------------\")\n",
    "        return all_merged_dfs\n",
    "    \n",
    "#Change path in here to read Ameriflux Hourly data from\n",
    "path = \"Ameriflux/AmeriFlux Hourly/\"\n",
    "#Change export path for all eeflux reference evapotranspiration \n",
    "eeflux_path = \"EEflux_refET_sites.csv\"\n",
    "am = AmerifluxHourlyRefET(path, eeflux_path)\n",
    "all_merged_list = am.prepare_data()\n",
    "#Change path in here for the joint data\n",
    "all_df = Helpers.list_to_df(all_merged_list)\n",
    "ameriflux_eeflux_path = \"Ameriflux/Generated/v1/Ameriflux_EEflux_refET\"\n",
    "Helpers.export_data(all_df, ameriflux_eeflux_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmerifluxErrorGraph Initializer\n",
      "   Site Id True Value Predicted Value    R2   RMSE    MSE   MAE  MAPE\n",
      "0   US-Tw1        etr      ETo EEflux -0.11   5.92   5.92  4.58   inf\n",
      "1   US-Skr        etr      ETo EEflux  0.00   4.89   4.89  4.62   NaN\n",
      "2   US-Snd        etr      ETo EEflux -0.52   8.15   8.15  6.86   inf\n",
      "3   US-Twt        etr      ETo EEflux -0.39   7.78   7.78  6.34   NaN\n",
      "4   US-Bi2        etr      ETo EEflux -0.08   7.52   7.52  6.14   inf\n",
      "5   US-Myb        etr      ETo EEflux  0.05   3.32   3.32  2.14   NaN\n",
      "6   US-FR2        etr      ETo EEflux -0.57   7.15   7.15  6.18   inf\n",
      "7   US-MMS        etr      ETo EEflux  0.49   1.86   1.86  1.37   inf\n",
      "8   US-Pon        etr      ETo EEflux  0.07   6.61   6.61  5.41   inf\n",
      "9   US-Me2        etr      ETo EEflux  0.02   6.20   6.20  4.73   NaN\n",
      "10  US-Ced        etr      ETo EEflux  0.00   4.71   4.71  3.75   NaN\n",
      "11  US-Kon        etr      ETo EEflux  0.02   7.18   7.18  5.81   inf\n",
      "12  US-KFS        etr      ETo EEflux -0.04   6.61   6.61  4.60   NaN\n",
      "13  US-ARM        etr      ETo EEflux -0.20   8.87   8.87  6.94   NaN\n",
      "14  US-Wlr        etr      ETo EEflux -0.15   7.04   7.04  5.83   NaN\n",
      "15  US-AR2        etr      ETo EEflux -0.67  12.59  12.59  9.52   NaN\n",
      "16  US-A32        etr      ETo EEflux  0.18   6.49   6.49  5.26   inf\n",
      "17  US-WBW        etr      ETo EEflux  0.00   3.88   3.88  3.41   NaN\n",
      "18  US-Tw3        etr      ETo EEflux -0.21   8.09   8.09  6.59   NaN\n",
      "19  US-Shd        etr      ETo EEflux -0.22   7.00   7.00  5.41   NaN\n",
      "20  US-SP2        etr      ETo EEflux -0.57   4.67   4.67  4.04   inf\n",
      "21  US-Var        etr      ETo EEflux -0.31   7.04   7.04  5.90   inf\n",
      "22  US-Me6        etr      ETo EEflux -0.50   7.44   7.44  5.56   NaN\n",
      "23  US-Goo        etr      ETo EEflux  0.04   4.84   4.84  4.12   NaN\n",
      "24  US-SO2        etr      ETo EEflux -0.61   5.45   5.45  4.57   NaN\n",
      "25  US-A74        etr      ETo EEflux -0.28   7.51   7.51  5.79   NaN\n",
      "26  US-AR1        etr      ETo EEflux -1.22  11.94  11.94  9.54   inf\n"
     ]
    }
   ],
   "source": [
    "full_path = os.path.join(base_path, \"Ameriflux/Generated/v1/Ameriflux_EEflux_refET\")\n",
    "df = Helpers.load_data(full_path)  \n",
    "df.head()\n",
    "\n",
    "class AmerifluxErrorGraph:\n",
    "    def __init__(self, path):\n",
    "        print(\"AmerifluxErrorGraph Initializer\")\n",
    "        self.path = path\n",
    "          \n",
    "    def read_data(self):\n",
    "        file_name = os.path.join(base_path, self.path)\n",
    "        df = pd.read_csv(file_name, index_col=None, header=0)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_error_metrics(self, y_true, y_predicted):\n",
    "        r2_Score = r2_score(y_true, y_predicted)\n",
    "        rmse_score = np.sqrt(mean_squared_error(y_true, y_predicted))\n",
    "        mse_score = mean_squared_error(y_true, y_predicted)\n",
    "        mae_score = mean_absolute_error(y_true, y_predicted)\n",
    "\n",
    "        def mean_absolute_percentage_error(y_true, y_pred):\n",
    "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mape_score = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "        num = 2\n",
    "        return (round(r2_Score, num), round(rmse_score, num), round(mse_score, num), \n",
    "                round(mae_score, num), round(mape_score, num))\n",
    "\n",
    "\n",
    "    def generate_errors(self, df, first_col, second_col):\n",
    "        df = df.replace(to_replace = np.nan, value =0) \n",
    "        errors = self.get_error_metrics(df[first_col], df[second_col])\n",
    "        return errors\n",
    "    \n",
    "    def plot_ref_et(self, df, site_id, first_column, second_column, path_to_save):\n",
    "        fig, ax = plt.subplots(figsize=(40, 17))\n",
    "        fig.subplots_adjust(bottom=0.15, left=0.2)\n",
    "        ax.plot(df[\"Date\"], df[first_column], marker='o', markersize=7.0, color='green', label=\"Ameriflux\")\n",
    "        ax.plot(df[\"Date\"], df[second_column], marker='^', markersize=7.0, color='red', label=\"EEflux\")\n",
    "        title = site_id + \": Comparison between \" + first_column + \" and \" + second_column\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=90)\n",
    "        ax.legend()\n",
    "#         plt.show()\n",
    "        full_path = os.path.join(base_path, path_to_save)\n",
    "        plt.savefig(full_path + site_id + \"_etr.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        unique_sites = df[\"Site Id\"].unique()\n",
    "        unique_sites\n",
    "\n",
    "        r2_list, rmse_list, mse_list, mae_list, mape_list = [], [], [], [], []\n",
    "        first_column = \"etr\"\n",
    "        second_column = \"ETo EEflux\"\n",
    "        for i in range(len(unique_sites)):\n",
    "            site_id = unique_sites[i]\n",
    "            df_site = df[df[\"Site Id\"] == site_id]\n",
    "            (r2, rmse, mse, mae, mape) = self.generate_errors(df_site, first_column, second_column)\n",
    "            r2_list.append(r2)\n",
    "            rmse_list.append(rmse)\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "            min_value = 1\n",
    "            max_value = 15\n",
    "            df_filtered_site = df_site[df_site[\"etr\"].between(left=min_value, right=max_value) &\n",
    "                                       df_site[\"ETo EEflux\"].between(left=min_value, right=max_value)]\n",
    "            \n",
    "            df_filtered_site['Date'] = pd.to_datetime(df_filtered_site[\"Date\"])\n",
    "            df_filtered_site.sort_values(by=\"Date\", inplace=True, ascending=True)\n",
    "            #Plot the output feature\n",
    "            if len(df_filtered_site) > 0:\n",
    "                self.plot_ref_et(df_filtered_site, site_id, first_column, second_column, \"Ameriflux/Generated/v1/Graphs/\")\n",
    "\n",
    "        df_errors = pd.DataFrame({\"Site Id\": unique_sites,\n",
    "                                  \"True Value\": first_column,\n",
    "                                  \"Predicted Value\": second_column,\n",
    "                                  \"R2\": r2_list,\n",
    "                                  \"RMSE\": rmse_list, \n",
    "                                  \"MSE\": rmse_list,\n",
    "                                  \"MAE\": mae_list, \n",
    "                                  \"MAPE\": mape_list})\n",
    "        print(df_errors)\n",
    "        errors_path = \"Ameriflux/Generated/v1/Errors_refET\"\n",
    "        Helpers.export_data(df_errors, errors_path)\n",
    "\n",
    "\n",
    "\n",
    "full_path = \"Ameriflux/Generated/v1/Ameriflux_EEflux_refET.csv\"\n",
    "am_eg = AmerifluxErrorGraph(full_path)\n",
    "df = am_eg.read_data()\n",
    "am_eg.prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
