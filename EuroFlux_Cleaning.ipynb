{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Helpers:\n",
    "    def __init__(self):\n",
    "        print(\"Helper\")\n",
    "        \n",
    "    def convert_missing_values_nan(df):\n",
    "        '''This function will convert -9999 to NaN'''\n",
    "        df = df.replace(-9999.000000, np.NaN)\n",
    "        return df\n",
    "\n",
    "    def drop_nan_columns(df):\n",
    "        '''Drops the columns having all theirs rows as Nans'''\n",
    "        columns_to_exclude = [\"Date\", \"Day\", \"Year\", \"Month\", \"Timestamp start\"\n",
    "                              , \"Time\", \"TIMESTAMP\", \"Tier\", \"TIMESTAMP_START\", \"TIMESTAMP_END\", \"Day Status\"]\n",
    "        columns = df.columns\n",
    "        for i in range(len(columns)):\n",
    "            col = columns[i]\n",
    "            if col in columns_to_exclude:\n",
    "                continue\n",
    "            nan_sum_col = df[col].isnull().sum()\n",
    "            if nan_sum_col == len(df):\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def drop_nans_rows(df):\n",
    "        '''This function will drop the rows having NaNs'''\n",
    "        print(\"Before removing missing values:\")\n",
    "        print(\"number of rows:\", df.shape[0], \"\\nnumber of columns:\", df.shape[1])\n",
    "        df = df.dropna(how='any')\n",
    "        print(\"After removing missing values:\")\n",
    "        print(\"number of rows:\", df.shape[0], \"\\nnumber of columns:\", df.shape[1])\n",
    "        return df\n",
    "        \n",
    "    def get_all_matching_columns(df, keyword):\n",
    "        return df.filter(like=keyword).columns\n",
    "\n",
    "    def generate_lags(df, column, lags_count): \n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "#             for j in range(i):\n",
    "#                 df.loc[str(j+1), lag_name] = np.nan\n",
    "#         df = df.dropna(how='any')\n",
    "        return df\n",
    "\n",
    "    def add_LE_conversion_rate(df, col):\n",
    "        conversion_rate = 28.94\n",
    "        new_col = col + \"(mm)\"\n",
    "        df[new_col] = df[col] / conversion_rate\n",
    "        return df\n",
    "\n",
    "    def read_sites_data():\n",
    "        file_path = os.path.join(base_path, \"filtered_sites_all.xlsx\")\n",
    "        df = pd.read_excel(file_path)\n",
    "        df.head()\n",
    "        return df\n",
    "\n",
    "    def export_data(df, file_path):\n",
    "        export_path = os.path.join(base_path, file_path + \".csv\")\n",
    "        export_csv = df.to_csv(export_path, index=None, header=True)\n",
    "\n",
    "    def load_data(file_path):\n",
    "        df = pd.read_csv(file_path + \".csv\", delimiter=',')\n",
    "        return df\n",
    "    \n",
    "    def list_to_df(list_to_convert):\n",
    "        '''This function will convert the provided list into a dataframe'''\n",
    "        df = pd.concat(list_to_convert, sort=True)\n",
    "        return df\n",
    "    \n",
    "    def get_files_directory(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "        listOfFile = os.listdir(dirName)\n",
    "        allFiles = list()\n",
    "        # Iterate over all the entries\n",
    "        for entry in listOfFile:\n",
    "            # Create full path\n",
    "            if entry.endswith(\".xlsx\") or entry.endswith(\".icloud\") or entry.endswith(\".DS_Store\"):\n",
    "                continue\n",
    "            fullPath = os.path.join(dirName, entry)\n",
    "            # If entry is a directory then get the list of files in this directory \n",
    "            if os.path.isdir(fullPath):\n",
    "                allFiles = allFiles + Helpers.get_files_directory(fullPath)\n",
    "            else:\n",
    "                allFiles.append(fullPath)\n",
    "\n",
    "        return allFiles\n",
    "\n",
    "    def concat_dataframe_from_files(files, skipRowsNum, split_num):\n",
    "        values = []\n",
    "        for i in range(len(files)):\n",
    "            file_path = files[i]\n",
    "            head, file_name = os.path.split(file_path)\n",
    "            #Get only the sheets having the variables\n",
    "            if file_name.endswith(\".csv\"):\n",
    "#                 print(\"file name\", file_name)\n",
    "                df = pd.read_csv(file_path, delimiter=',', skiprows=skipRowsNum)\n",
    "                site_id = file_name.split(\"_\")[split_num]\n",
    "#                 print(\"site id in file:\", site_id)\n",
    "                df[\"Site Id\"] = site_id\n",
    "                values.append(df)\n",
    "        return Helpers.list_to_df(values)   \n",
    "    \n",
    "    def generate_dataframe_from_files(dirName, skipRowsNum = 0, split_num = 0):\n",
    "        files = Helpers.get_files_directory(dirName)\n",
    "        df = Helpers.concat_dataframe_from_files(files, skipRowsNum, split_num)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer\n",
      "L2 file: EFDC_L2_Flx_FIJok_2002_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCA2_2014_v014_30m.txt\n",
      "L2 file: EFDC_L2_Flx_IECa1_2008_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2010_v030_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2007_v09_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2013_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRAvi_2004_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_IECa1_2004_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCA2_2012_v015_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCA2_2011_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2014_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2006_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2015_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCas_2007_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCA2_2013_v017_30m.txt\n",
      "L2 file: EFDC_L2_Flx_IECa1_2007_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2008_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCas_2008_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2009_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2016_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FIJok_2001_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRAvi_2006_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ESES2_2009_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2015_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2008_v010_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FIJok_2000_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2012_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_IECa1_2005_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCas_2009_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_IECa1_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2014_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCas_2010_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FIJok_2003_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITCas_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2013_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2007_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2004_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2004_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_BELon_2018_v08_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2011_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2005_v011_30m.txt\n",
      "L2 file: EFDC_L2_Flx_DERuS_2018_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ESES2_2010_v06_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2009_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2010_v08_30m.txt\n",
      "L2 file: EFDC_L2_Flx_BELon_2017_v010_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2012_v09_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2011_v012_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITBCi_2005_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2005_v011_1hr.txt\n",
      "L2 file: EFDC_L2_Flx_FRGri_2017_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_FRAvi_2005_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_UKESa_2005_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_UKESa_2003_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLLan_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2007_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_UKHer_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2009_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLLan_2005_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2008_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2012_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_UKESa_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2011_v07_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2008_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2012_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2007_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLLut_2007_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLLut_2006_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2013_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2009_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2011_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLMol_2006_v01_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo4_2013_v05_30m.txt\n",
      "L2 file: EFDC_L2_Flx_NLMol_2005_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_UKESa_2004_v02_30m.txt\n",
      "L2 file: EFDC_L2_Flx_ITRo3_2010_v03_30m.txt\n",
      "L2 file: EFDC_L2_Flx_CHOe2_2017_v04_30m.txt\n",
      "L2 file: EFDC_L2_Flx_CHOe2_2018_v05_30m.txt\n",
      "-----------------------------------------\n",
      "L3 file: CEIP_EC_L3_UKESa_2005_v01.txt\n",
      "L3 file: CEIP_EC_L3_UKESa_2004_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLLan_2005_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2009_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCA2_2012_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo4_2008_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLLan_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_UKESa_2003_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCas_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCas_2007_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLLut_2007_v02.txt\n",
      "L3 file: CEIP_EC_L3_IECa1_2007_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLMol_2005_v02.txt\n",
      "L3 file: CEIP_EC_L3_IECa1_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCas_2008_v02.txt\n",
      "L3 file: CEIP_EC_L3_IECa1_2004_v01.txt\n",
      "L3 file: CEIP_EC_L3_FRAvi_2004_v02.txt\n",
      "L3 file: CEIP_EC_L3_FRAvi_2005_v02.txt\n",
      "L3 file: CEIP_EC_L3_IECa1_2005_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCas_2009_v02.txt\n",
      "L3 file: CEIP_EC_L3_ESES2_2009_v02.txt\n",
      "L3 file: CEIP_EC_L3_ITRo3_2008_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo3_2009_v01.txt\n",
      "L3 file: CEIP_EC_L3_UKHer_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo3_2011_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo3_2013_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2008_v02.txt\n",
      "L3 file: CEIP_EC_L3_ITRo4_2009_v02.txt\n",
      "L3 file: CEIP_EC_L3_ESES2_2010_v02.txt\n",
      "L3 file: CEIP_EC_L3_ITRo3_2012_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITCas_2010_v02.txt\n",
      "L3 file: CEIP_EC_L3_FRGri_2004_v01.txt\n",
      "L3 file: CEIP_EC_L3_FRGri_2005_v01.txt\n",
      "L3 file: CEIP_EC_L3_FRGri_2007_v01.txt\n",
      "L3 file: CEIP_EC_L3_FRGri_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLLut_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_FRAvi_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_NLMol_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2005_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo4_2011_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2010_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2004_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2006_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo4_2013_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITRo4_2012_v01.txt\n",
      "L3 file: CEIP_EC_L3_ITBCi_2007_v01.txt\n",
      "-----------------------------------------\n",
      "L4 file: CEIP_EC_L4_h_ITRo4_2008_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCA2_2012_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2009_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ESES2_2010_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLLan_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLLan_2005_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCas_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLLut_2007_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCas_2007_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_FRGri_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_UKESa_2003_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCas_2008_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_FRAvi_2004_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_FRAvi_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCas_2009_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_IECa1_2007_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_IECa1_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLMol_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo3_2008_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo3_2009_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2004_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo3_2013_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo4_2009_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2008_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo3_2012_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_UKESa_2004_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_UKHer_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo3_2011_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_UKESa_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_FRGri_2007_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_FRGri_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLLut_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITCas_2010_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_FRGri_2004_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_NLMol_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_IECa1_2005_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_IECa1_2004_v02.txt\n",
      "L4 file: CEIP_EC_L4_h_FRAvi_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo4_2013_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2006_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2007_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo4_2012_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ESES2_2009_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITBCi_2010_v01.txt\n",
      "L4 file: CEIP_EC_L4_h_ITRo4_2011_v01.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>DoY</th>\n",
       "      <th>GPP_or_ANN</th>\n",
       "      <th>GPP_or_MDS</th>\n",
       "      <th>GPP_st_ANN</th>\n",
       "      <th>GPP_st_MDS</th>\n",
       "      <th>H_f</th>\n",
       "      <th>H_fqc</th>\n",
       "      <th>Hour</th>\n",
       "      <th>LE_f</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta_f</th>\n",
       "      <th>Ta_fqc</th>\n",
       "      <th>Ts_f</th>\n",
       "      <th>Ts_fqc</th>\n",
       "      <th>VPD_f</th>\n",
       "      <th>VPD_fqc</th>\n",
       "      <th>Version</th>\n",
       "      <th>Year</th>\n",
       "      <th>qf_NEE_or</th>\n",
       "      <th>qf_NEE_st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.021</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-26.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.19</td>\n",
       "      <td>...</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>v01.txt</td>\n",
       "      <td>2008</td>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-25.78</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>v01.txt</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-29.29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>v01.txt</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.083</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-25.95</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>v01.txt</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.104</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-33.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.72</td>\n",
       "      <td>...</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>v01.txt</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day    DoY  GPP_or_ANN  GPP_or_MDS  GPP_st_ANN  GPP_st_MDS    H_f  H_fqc  \\\n",
       "0    1  1.021      -0.141      -0.029      -0.248      -0.026 -26.67      0   \n",
       "1    1  1.042       0.387       0.387       0.391       0.391 -25.78      0   \n",
       "2    1  1.063       0.225       0.225       0.229       0.229 -29.29      0   \n",
       "3    1  1.083       0.321       0.321       0.325       0.325 -25.95      0   \n",
       "4    1  1.104       0.225       0.225       0.229       0.229 -33.06      0   \n",
       "\n",
       "   Hour  LE_f  ...  Ta_f  Ta_fqc  Ts_f  Ts_fqc   VPD_f  VPD_fqc  Version  \\\n",
       "0   0.5  2.19  ...  8.12       0  8.31       0 -9999.0    -9999  v01.txt   \n",
       "1   1.0  2.06  ...  7.78       0  8.25       0 -9999.0    -9999  v01.txt   \n",
       "2   1.5  1.25  ...  7.43       0  8.20       0 -9999.0    -9999  v01.txt   \n",
       "3   2.0  1.05  ...  7.62       0  8.14       0 -9999.0    -9999  v01.txt   \n",
       "4   2.5  0.72  ...  7.31       0  8.10       0 -9999.0    -9999  v01.txt   \n",
       "\n",
       "   Year  qf_NEE_or  qf_NEE_st  \n",
       "0  2008        259        259  \n",
       "1  2008          0          0  \n",
       "2  2008          0          0  \n",
       "3  2008          0          0  \n",
       "4  2008          0          0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EuroFlux:\n",
    "\n",
    "    def __init__(self, input_path, output_path):\n",
    "        print(\"Initializer\")\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        \n",
    "    def unzip_folder(self, filename):\n",
    "        if filename.endswith(\".zip\"):\n",
    "            name = os.path.splitext(os.path.basename(filename))[0]\n",
    "            path = self.input_path + name\n",
    "            if not os.path.isdir(path):\n",
    "                try:\n",
    "                    zip = zipfile.ZipFile(filename)\n",
    "\n",
    "                    os.mkdir(path)\n",
    "                    zip.extractall(path=path)\n",
    "                    zip.close()\n",
    "                    os.remove(filename)\n",
    "                except:\n",
    "                    print(\"BAD ZIP: \", filename)\n",
    "        \n",
    "    def unzip_folders(self):\n",
    "        files = Helpers.get_files_directory(input_path)\n",
    "        for i in range(len(files)):\n",
    "            file = files[i]\n",
    "            head, file_name = os.path.split(file)\n",
    "            if file_name.endswith(\".zip\"):\n",
    "                self.unzip_folder(file)\n",
    "                \n",
    "    def read_l2_data(self):\n",
    "        files = Helpers.get_files_directory(input_path)\n",
    "        l2_files = []\n",
    "        df_list = []\n",
    "        for i in range(len(files)):\n",
    "            file = files[i]\n",
    "            head, file_name = os.path.split(file)\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                components = file_name.split(\"_\")\n",
    "                if components[1] == \"L2\":\n",
    "                    print(\"L2 file:\", file_name)\n",
    "                    l2_files.append(file_name)\n",
    "                    df = pd.read_csv(file , sep=\",\")\n",
    "                    df[\"Site Id\"] = components[3]\n",
    "                    df[\"Year\"] = components[4]\n",
    "                    df[\"Version\"] = components[5]\n",
    "                    df_list.append(df)\n",
    "                    \n",
    "        return Helpers.list_to_df(df_list)\n",
    "\n",
    "    def read_l3_data(self):\n",
    "        files = Helpers.get_files_directory(input_path)\n",
    "        l2_files = []\n",
    "        df_list = []\n",
    "        for i in range(len(files)):\n",
    "            file = files[i]\n",
    "            head, file_name = os.path.split(file)\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                components = file_name.split(\"_\")\n",
    "                if len(components) > 2:\n",
    "                    if components[2] == \"L3\":\n",
    "                        print(\"L3 file:\", file_name)\n",
    "                        l2_files.append(file_name)\n",
    "                        df = pd.read_csv(file , sep=\",\")\n",
    "                        df[\"Site Id\"] = components[3]\n",
    "                        df[\"Year\"] = components[4]\n",
    "                        df[\"Version\"] = components[5]\n",
    "                        df_list.append(df)\n",
    "                    \n",
    "        return Helpers.list_to_df(df_list)\n",
    "    \n",
    "    def read_l4_data(self):\n",
    "        files = Helpers.get_files_directory(input_path)\n",
    "        l2_files = []\n",
    "        df_list = []\n",
    "        for i in range(len(files)):\n",
    "            file = files[i]\n",
    "            head, file_name = os.path.split(file)\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                components = file_name.split(\"_\")\n",
    "                if len(components) > 2:\n",
    "                    if (components[2] == \"L4\") and (components[3] == \"h\"):\n",
    "                        print(\"L4 file:\", file_name)\n",
    "                        l2_files.append(file_name)\n",
    "                        df = pd.read_csv(file , sep=\",\")\n",
    "                        df[\"Site Id\"] = components[4]\n",
    "                        df[\"Year\"] = components[5]\n",
    "                        df[\"Version\"] = components[6]\n",
    "                        df_list.append(df)\n",
    "                    \n",
    "        return Helpers.list_to_df(df_list)\n",
    "                    \n",
    "input_path = \"/Users/saraawad/Desktop/Datasets/Google/Euroflux/\"\n",
    "output_path = \"\"\n",
    "ef = EuroFlux(input_path, output_path)\n",
    "ef.unzip_folders()\n",
    "df2_list = ef.read_l2_data()\n",
    "df2_list.head()\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "df3_list = ef.read_l3_data()\n",
    "df3_list.head()\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "df4_list = ef.read_l4_data()\n",
    "df4_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/Users/saraawad/Desktop/Datasets/Google/\"\n",
    "Helpers.export_data(df2_list, \"EuroFlux/Processed Data/L2\")\n",
    "Helpers.export_data(df3_list, \"EuroFlux/Processed Data/L3\")\n",
    "Helpers.export_data(df4_list, \"EuroFlux/Processed Data/L4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
